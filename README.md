# Azure Databricks Retail Lakehouse

End-to-end lakehouse project built on **Azure Databricks**, demonstrating:
- Raw data ingestion (Bronze)
- Transformations & cleaning (Silver)
- Aggregations & business metrics (Gold)
- Machine Learning (forecasting & pricing)
- Visualization with Databricks SQL

## ğŸ—ï¸ Project Structure

azure-databricks-retail-lakehouse/
â”œâ”€ src/                        # All your Databricks jobs/notebooks
â”‚   â”œâ”€ ingestion/              # Bronze: raw ingestion scripts
â”‚   â”œâ”€ transformations/        # Silver: cleaning + joins
â”‚   â”œâ”€ gold/                   # Gold: aggregates, business-ready tables
â”‚   â””â”€ ml/                     # Machine learning (forecasting/pricing)
â”œâ”€ infra/                      # Infrastructure & config
â”‚   â”œâ”€ unity_catalog_setup.sql # Catalog & schema creation
â”‚   â””â”€ storage_config.json     # Storage mount/config details
â”œâ”€ jobs/                       # Workflow/job JSON definitions
â”‚   â””â”€ workflows.json
â”œâ”€ dbsql/                      # SQL queries & dashboard notes
â”‚   â””â”€ dashboards.md
â”œâ”€ tests/                      # Unit tests for PySpark transforms
â”‚   â””â”€ test_transforms.py
â”œâ”€ data/                       # Only small local test CSVs (not prod data)
â”œâ”€ README.md                   # Project overview

## ğŸš€ Tech Stack
- Azure Databricks
- Azure Data Lake Gen2
- Unity Catalog
- PySpark / Delta Lake
- MLflow
- Databricks SQL

## ğŸ“Š Dataset
[Kaggle: Store Item Demand Forecasting Challenge](https://www.kaggle.com/competitions/demand-forecasting-kernels-only/data)

## ğŸ”— Links
- [Project GitHub Repo](#)
- [LinkedIn Project Post](#)

â”œâ”€ requirements.txt            # Python deps (mlflow, pandas, etc.)
â””â”€ .github/workflows/ci.yml    # CI/CD pipeline (lint/tests/deploy)
